{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **DeepLab V3+**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom skimage.io import imread\nfrom torch.utils import data\nfrom torch.utils.data import DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.tensorboard import SummaryWriter\nimport pathlib\nimport torchvision.transforms.functional as fn\nfrom torchvision.io import read_image\nfrom torchvision import transforms\nimport os\nimport matplotlib.pyplot as plt\nimport cv2 as cv\nfrom tqdm import tqdm\nimport numpy as np\nimport re\nimport sys","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install segmentation-models-pytorch","metadata":{"execution":{"iopub.status.busy":"2022-09-23T14:33:57.544718Z","iopub.execute_input":"2022-09-23T14:33:57.545295Z","iopub.status.idle":"2022-09-23T14:34:11.113958Z","shell.execute_reply.started":"2022-09-23T14:33:57.545259Z","shell.execute_reply":"2022-09-23T14:34:11.112664Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Collecting segmentation-models-pytorch\n  Using cached segmentation_models_pytorch-0.3.0-py3-none-any.whl (97 kB)\nCollecting pretrainedmodels==0.7.4\n  Using cached pretrainedmodels-0.7.4-py3-none-any.whl\nRequirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from segmentation-models-pytorch) (9.1.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from segmentation-models-pytorch) (4.64.0)\nRequirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from segmentation-models-pytorch) (0.12.0+cpu)\nCollecting efficientnet-pytorch==0.7.1\n  Using cached efficientnet_pytorch-0.7.1-py3-none-any.whl\nCollecting timm==0.4.12\n  Using cached timm-0.4.12-py3-none-any.whl (376 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.11.0+cpu)\nRequirement already satisfied: munch in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (2.5.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.21.6)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (4.3.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (2.28.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from munch->pretrainedmodels==0.7.4->segmentation-models-pytorch) (1.15.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2022.6.15.2)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (1.26.12)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (3.3)\nInstalling collected packages: efficientnet-pytorch, timm, pretrainedmodels, segmentation-models-pytorch\nSuccessfully installed efficientnet-pytorch-0.7.1 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.0 timm-0.4.12\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --upgrade certifi","metadata":{"execution":{"iopub.status.busy":"2022-09-23T14:39:34.468590Z","iopub.execute_input":"2022-09-23T14:39:34.469127Z","iopub.status.idle":"2022-09-23T14:40:01.887588Z","shell.execute_reply.started":"2022-09-23T14:39:34.469080Z","shell.execute_reply":"2022-09-23T14:40:01.886031Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (2022.6.15.2)\nCollecting certifi\n  Downloading certifi-2022.9.14-py3-none-any.whl (162 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.5/162.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: certifi\n  Attempting uninstall: certifi\n    Found existing installation: certifi 2022.6.15.2\n    Uninstalling certifi-2022.6.15.2:\n      Successfully uninstalled certifi-2022.6.15.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbeatrix-jupyterlab 3.1.7 requires google-cloud-bigquery-storage, which is not installed.\npandas-profiling 3.1.0 requires markupsafe~=2.0.1, but you have markupsafe 2.1.1 which is incompatible.\ngoogle-api-core 1.33.0 requires protobuf<4.0.0dev,>=3.20.1, but you have protobuf 3.19.4 which is incompatible.\ngcsfs 2022.5.0 requires fsspec==2022.5.0, but you have fsspec 2022.8.2 which is incompatible.\napache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.5.1 which is incompatible.\napache-beam 2.40.0 requires pyarrow<8.0.0,>=0.15.1, but you have pyarrow 8.0.0 which is incompatible.\nallennlp 2.10.0 requires protobuf==3.20.0, but you have protobuf 3.19.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed certifi-2022.9.14\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"transform = transforms.Grayscale()","metadata":{"execution":{"iopub.status.busy":"2022-09-23T14:40:47.974753Z","iopub.execute_input":"2022-09-23T14:40:47.975225Z","iopub.status.idle":"2022-09-23T14:40:47.981522Z","shell.execute_reply.started":"2022-09-23T14:40:47.975171Z","shell.execute_reply":"2022-09-23T14:40:47.980311Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"class SegmentationDataSet2(data.Dataset):\n    \"\"\"Image segmentation dataset with caching and pretransforms.\"\"\"\n    def __init__(self,\n                 inputs: list,\n                 targets: list,\n                 transform=None,\n                 use_cache=False,\n                 pre_transform=None,\n                 ):\n        self.inputs = inputs\n        self.targets = targets\n        self.transform = transform\n        self.inputs_dtype = torch.float32\n        self.targets_dtype = torch.float32\n        self.use_cache = use_cache\n        self.pre_transform = pre_transform\n\n        if self.use_cache:\n            self.cached_data = []\n\n            progressbar = tqdm(range(len(self.inputs)), desc='Caching')\n            for i, img_name, tar_name in zip(progressbar, self.inputs, self.targets):\n                img, tar = imread(str(img_name)), imread(str(tar_name))\n                if self.pre_transform is not None:\n                    img, tar = self.pre_transform(img, tar)\n\n                self.cached_data.append((img, tar))\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self,\n                    index: int):\n        if self.use_cache:\n            x, y = self.cached_data[index]\n        else:\n            # Select the sample\n            input_ID = self.inputs[index]\n            target_ID = self.targets[index]\n\n            # Load input and target\n            x, y = read_image(input_ID), read_image(target_ID)\n            x, y = x.type(self.inputs_dtype), y.type(self.targets_dtype)\n            x = (x - torch.min(x)) / (torch.max(x)-torch.min(x))\n            y = transform(y)\n            x,y = fn.resize(x,size=[512,512]),fn.resize(y,size=[512,512])\n            y[y>0]=1\n\n        # Preprocessing\n        if self.transform is not None:\n            x, y = self.transform(x, y)\n\n        return x, y","metadata":{"execution":{"iopub.status.busy":"2022-09-23T14:34:15.052405Z","iopub.execute_input":"2022-09-23T14:34:15.053690Z","iopub.status.idle":"2022-09-23T14:34:15.071605Z","shell.execute_reply.started":"2022-09-23T14:34:15.053614Z","shell.execute_reply":"2022-09-23T14:34:15.070030Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"**Load the data**","metadata":{}},{"cell_type":"code","source":"inputs = []\ndir1 =  '../input/final-contrail-dataset/FinalData/FalseColor'\nfor root, dirs, files in os.walk(dir1):\n    inputs.extend(files)\ninputs = sorted(inputs)\n        \n# read annotations\ntarget = []\ndir2 = '../input/final-contrail-dataset/FinalData/Contrail'\nfor root, dirs, files in os.walk(dir2):\n    target.extend(files)\ntarget = sorted(target)\ninp = []\ntar = []\nfor x in inputs:\n    inp.append('../input/final-contrail-dataset/FinalData/FalseColor/'+x)\nfor x in target:\n    tar.append('../input/final-contrail-dataset/FinalData/Contrail/'+x)\ninputs = inp\ntarget = tar","metadata":{"execution":{"iopub.status.busy":"2022-09-23T14:34:16.640393Z","iopub.execute_input":"2022-09-23T14:34:16.641930Z","iopub.status.idle":"2022-09-23T14:34:18.514155Z","shell.execute_reply.started":"2022-09-23T14:34:16.641868Z","shell.execute_reply":"2022-09-23T14:34:18.512750Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"inp[0:5]","metadata":{"execution":{"iopub.status.busy":"2022-09-23T14:34:18.515881Z","iopub.execute_input":"2022-09-23T14:34:18.516469Z","iopub.status.idle":"2022-09-23T14:34:18.527678Z","shell.execute_reply.started":"2022-09-23T14:34:18.516433Z","shell.execute_reply":"2022-09-23T14:34:18.526174Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"['../input/final-contrail-dataset/FinalData/FalseColor/1fci0.jpg',\n '../input/final-contrail-dataset/FinalData/FalseColor/1fci1.jpg',\n '../input/final-contrail-dataset/FinalData/FalseColor/1fci10.jpg',\n '../input/final-contrail-dataset/FinalData/FalseColor/1fci100.jpg',\n '../input/final-contrail-dataset/FinalData/FalseColor/1fci101.jpg']"},"metadata":{}}]},{"cell_type":"markdown","source":"**Remove all the images and masks that thave no contrails**","metadata":{}},{"cell_type":"code","source":"input_true = []\ntarget_true = []\nfor count, value in enumerate(tar):\n    y = read_image(value)\n    y = transform(y)\n    y[y>0]=1 \n    if(len(y.unique())>1):\n        input_true.append(inp[count])\n        target_true.append(tar[count])\n    if(count%100==0):\n        print(count)","metadata":{"execution":{"iopub.status.busy":"2022-09-23T14:34:18.529823Z","iopub.execute_input":"2022-09-23T14:34:18.530817Z","iopub.status.idle":"2022-09-23T14:37:55.896335Z","shell.execute_reply.started":"2022-09-23T14:34:18.530763Z","shell.execute_reply":"2022-09-23T14:37:55.895143Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\n1100\n1200\n1300\n1400\n1500\n1600\n1700\n1800\n1900\n2000\n2100\n2200\n2300\n2400\n2500\n2600\n2700\n2800\n2900\n3000\n3100\n3200\n3300\n3400\n3500\n3600\n3700\n3800\n3900\n4000\n4100\n4200\n","output_type":"stream"}]},{"cell_type":"code","source":"len(input_true)","metadata":{"execution":{"iopub.status.busy":"2022-09-23T14:37:55.899205Z","iopub.execute_input":"2022-09-23T14:37:55.899680Z","iopub.status.idle":"2022-09-23T14:37:55.907424Z","shell.execute_reply.started":"2022-09-23T14:37:55.899627Z","shell.execute_reply":"2022-09-23T14:37:55.906044Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"2171"},"metadata":{}}]},{"cell_type":"markdown","source":"**Train Test split and Dataloader**","metadata":{}},{"cell_type":"code","source":"writer = SummaryWriter()\n\nrandom_seed = 60\n\n# split dataset into training set and validation set\ntrain_size = 0.8  # 80:20 split\n\ninputs_train, inputs_valid = train_test_split(\n    input_true,\n    random_state=random_seed,\n    train_size=train_size,\n    shuffle=True)\n\ntargets_train, targets_valid = train_test_split(\n    target_true,\n    random_state=random_seed,\n    train_size=train_size,\n    shuffle=True)\n\n# dataset training\ndataset_train = SegmentationDataSet2(inputs=inputs_train,\n                                    targets=targets_train,\n                                    transform=None)\n\n# dataset validation\ndataset_valid = SegmentationDataSet2(inputs=inputs_valid,\n                                    targets=targets_valid,\n                                    transform=None)\n\n# dataloader training\ndataloader_training = DataLoader(dataset=dataset_train,\n                                 batch_size=4,\n                                 shuffle=True)\n\n# dataloader validation\ndataloader_validation = DataLoader(dataset=dataset_valid,\n                                   batch_size=4,\n                                   shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-23T14:39:00.814433Z","iopub.execute_input":"2022-09-23T14:39:00.814933Z","iopub.status.idle":"2022-09-23T14:39:00.829543Z","shell.execute_reply.started":"2022-09-23T14:39:00.814893Z","shell.execute_reply":"2022-09-23T14:39:00.828270Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"##Trying out the data loaded\nbatch = dataset_train[114]\nx, y = batch\n\nprint(f'x = shape: {x.shape}; type: {x.dtype}')\nprint(f'x = min: {x.min()}; max: {x.max()}')\nprint(f'y = shape: {y.shape}; class: {y.unique()}; type: {y.dtype}')\nprint(f'y = min: {y.min()}; max: {y.max()}')\n#Getting the next batch::  x, y = next(iter(dataloader_training))","metadata":{"execution":{"iopub.status.busy":"2022-09-23T14:39:08.309705Z","iopub.execute_input":"2022-09-23T14:39:08.310178Z","iopub.status.idle":"2022-09-23T14:39:08.460796Z","shell.execute_reply.started":"2022-09-23T14:39:08.310142Z","shell.execute_reply":"2022-09-23T14:39:08.459850Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"x = shape: torch.Size([3, 512, 512]); type: torch.float32\nx = min: 0.0; max: 1.0\ny = shape: torch.Size([1, 512, 512]); class: tensor([0., 1.]); type: torch.float32\ny = min: 0.0; max: 1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"x,y = dataset_valid[114]\n# print(y.shape)\nplt.imshow(y.reshape(512,512), cmap='gray')\n#y[1]\n#plt.imshow(x.permute(1, 2, 0))","metadata":{"execution":{"iopub.status.busy":"2022-09-23T14:39:08.787721Z","iopub.execute_input":"2022-09-23T14:39:08.788932Z","iopub.status.idle":"2022-09-23T14:39:09.190466Z","shell.execute_reply.started":"2022-09-23T14:39:08.788869Z","shell.execute_reply":"2022-09-23T14:39:09.189584Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"<matplotlib.image.AxesImage at 0x7f2fc8280810>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAR0UlEQVR4nO3df4xV5Z3H8feHmcGBHRDlx4QyZAdbokXjskBaW5ut2rihthFtaJcGq1ZS0t3YH+kmimu66zZr449U2iatLNF2YdPWskULMWWRosT+UwsIlV9r+RGEQRkCDmMZAgzMd/+4z0wHHnUuMHfuvfB5JU/uOc95zj3fgZnPnHPuOWcUEZiZ9Tao3AWYWeVxMJhZxsFgZhkHg5llHAxmlnEwmFmmJMEgabqk1yXtkDSvFNsws9JRf1/HIKkG+BNwM9ACrAW+GBFb+3VDZlYypdhj+AiwIyJ2RcQJ4BlgRgm2Y2YlUluC9xwH7O013wJ89P1WkOTLL81K72BEjC5mYCmCoSiS5gJzy7V9s4vQG8UOLEUw7APG95pvSn2niYiFwELwHoNZpSnFOYa1wERJEyQNBmYBy0uwHTMrkX7fY4iIk5LuBVYCNcBPImJLf2/HzEqn3z+uPKcifChhNhDWR8S0Ygb6ykczyzgYzCzjYDCzjIPBzDIOBjPLOBjMLONgMLOMg8HMMg4GM8s4GMws42Aws4yDwcwyDgYzyzgYzCzjYDCzjIPBzDIOBjPLOBjMLONgMLOMg8HMMg4GM8s4GMws42Aws4yDwcwyDgYzyzgYzCzjYDCzjIPBzDIOBjPLOBjMLONgMLOMg8HMMg4GM8s4GMws02cwSPqJpAOSNvfqu1zSKknb0+tlqV+Sfihph6TXJE0pZfFmVhrF7DH8FzD9jL55wOqImAisTvMAnwYmpjYXeLJ/yjSzgdRnMETEy8DbZ3TPABal6UXAbb36F0fB74ERksb2U61mNkDO9RxDY0S8lab3A41pehywt9e4ltSXkTRX0jpJ686xBjMrkdrzfYOICElxDustBBYCnMv6ZlY657rH0Np9iJBeD6T+fcD4XuOaUp+ZVZFzDYblwF1p+i5gWa/+O9OnE9cB7b0OOcysWkTE+zbgF8BbQCeFcwZzgJEUPo3YDvwWuDyNFfAjYCewCZjW1/un9cLNza3kbV0xP48RgdIPZln5HIPZgFgfEdOKGegrH80s42Aws4yDwcwyDgYzyzgYzCzjYDCzjIPBzDIOBjPLOBjMLONgMLOMg8HMMg4GM8s4GMws42Aws4yDwcwyDgYzyzgYzCzjYDCzjIPBzDIOBjPLOBjMLONgMLOMg8HMMg4GM8s4GMws42Aws4yDwcwyDgYzyzgYzCzjYDCzjIPBzDIOBjPLOBjMLNNnMEgaL+klSVslbZH0jdR/uaRVkran18tSvyT9UNIOSa9JmlLqL8LM+ldtEWNOAv8cEa9KGgasl7QKuBtYHRGPSJoHzAPuBz4NTEzto8CT6dXKpKmpiblz5zJ06NCevh//+Mfs2rWrjFVZJeszGCLiLeCtNP1nSduAccAM4IY0bBGwhkIwzAAWR0QAv5c0QtLY9D42AEaOHMmUKVPo6uqitraW22+/nXvuuYfDhw+zceNGgNNCwiwTEUU3oBnYAwwHDvfqV/c88DzwiV7LVgPT+njfcDv/NmLEiBg1alR8/vOfj4iILVu2xKlTpyIi4uTJk7F48eKy1+hW1rau2J/1Yg4lAJDUACwFvhkR70jqWRYRISmKfa/0fnOBuWezjr23mpoannvuOa655hrq6uoAmDRpEgDt7e2sWLGCr3/96+Us0apIUcEgqY5CKPwsIp5N3a3dhwiSxgIHUv8+YHyv1ZtS32kiYiGwML3/WYWKFYKgpqaG5uZmvvvd79LQ0MDUqVMZNmwYEUFnZye1tbU8+OCDrFixgtraWtrb28tdtlWLIg4fBCwGvn9G/+PAvDQ9D3gsTX8GWJHWuw74QxHbKPcuVlW1urq6WLBgQWzdujXefPPNONPvfve7mDx5ctx3330xbNiwstfrVjGtXw8lrge+BGyStDH1/QvwCLBE0hzgDeALadlvgFuAHcBR4MtFbMOKUFdXx8yZM7n22muZPXs2DQ0NPcu6urpYsmQJ27dv57LLLmPjxo09JxrNzpbSb+zyFuFDiff1wQ9+kMbGRoYPH87SpUtP+0Th1KlTtLe388Ybb/C5z32O3bt3M2jQILq6uspYsVWo9RExrZiBRZ98tPIYPXo0s2bNYvbs2QwbNqwnFI4cOcKuXbvYtm0bDz30EK2trbS1tQE4FOz8nc3HlaVqlP/YqyLbkCFDYv369fHOO++cdg7h0KFDce+990ZDQ0M0NDR0fyLk5tZX6/+PK23gjBkzhp/+9KcMHz6cSZMmUV9f37Osra2NmTNnsmbNmu5QNet3DoYKUV9fz913383IkSOZPHky06dPZ9Cgv9zKsnXrVp599lmOHz/O5s2bHQpWUg6GMquvr+eaa66hoaGBr33taz0XJQFEBJs2beL48eOsXLmSb3/722Ws1C4mDoYyGjp0KPfccw9PPPEEtbW1dF9NeuzYMVpaWjhx4gTTp09n//79Za7ULjrlPvF4MZ98vP3226Otre20E4vbtm2LNWvWRHNzc4waNarsNbpdUM0nH6vBsWPHOHToEJdeeikdHR08/PDDvPDCC+zZs4eDBw+Wuzy7iPkCpzIaM2YMzc3NPfO7d+/mwIED772C2fkp+gInB4PZxaPoYPAzH80s42Aws4yDwcwyDgYzyzgYzCzjYDCzjIPBzDIOBjPLOBjMLONgMLOMg8HMMg6GXmbMmMGTTz7JjTfeSG2tbzy1i9dF/d0/ePBgPv7xj9PZ2QnAzJkzueOOO7j++uv55Cc/2fPUZbOLzUUZDI2NjdTU1HDzzTezYMECurq66Orqoq6ujjfffJNly5Zx7NixcpdpVj7lfnrTQD/BaciQIbFhw4Y4fPhwdHR09Dw5qa2tLd5+++244447or6+3o9kd7sQm5/g1Nv8+fOZOnUqAJK46qqreh7J3trayp133klLSwsNDQ0cPXrUewt20btgg6GmpoavfOUrjB49mltvvZUrrriiZ9nRo0f59a9/zaZNm2hvb2fNmjWcOHGijNWaVZYLLhgaGxuZMGECtbW13H///ac9Ou3IkSNs2bKFrVu38vDDD7Nz587yFWpWwS6YYGhsbOSGG27gyiuv5KGHHgIKhw3Hjx+ns7OTjo4Oli5dyiOPPEJLS4v/YIvZ+7ggguEDH/gAK1eu5Oqrr6ajo6Pn7zMcOXKERx99lMWLFwNw6NAhOjo6ylmqWVWo6mAYNGgQH/7wh3nqqae4+uqrgcK1Ca+++iqPP/44u3fv5uDBg+zZs6fMlZpVl6oNhunTp/Od73yH4cOHc+WVV3LkyBG++tWvsmXLFlpbW9m/f78PF8zOUdUGQ2dnZ89ewve+9z1aW1t5+eWX2bt3b5krM6t+VRsMdXV1rF27loMHD/LYY4/5D7WY9aM+/+CMpHrgZeASCkHyq4j4N0kTgGeAkcB64EsRcULSJcBiYCpwCPiHiNjdxza8z29Wev36B2eOAzdFxN8Ak4Hpkq4DHgXmR8SHgDZgTho/B2hL/fPTODOrIn0GQ7qV4EiarUstgJuAX6X+RcBtaXpGmict/5S6Pz80s6pQ1PMYJNVI2ggcAFYBO4HDEXEyDWkBxqXpccBegLS8ncLhxpnvOVfSOknrzusrMLN+V1QwRMSpiJgMNAEfAa463w1HxMKImFbsMY+ZDZyzeoJTRBwGXgI+BoyQ1P2pRhOwL03vA8YDpOWXUjgJaWZVos9gkDRa0og0PQS4GdhGISBmpmF3AcvS9PI0T1r+YvhKI7OqUsx1DGOBRZJqKATJkoh4XtJW4BlJ/wFsAJ5O458G/lvSDuBtYFYJ6jazEurzOoYBKcLXMZgNhH69jsHMLjIOBjPLOBjMLONgMLOMg8HMMg4GM8s4GMws42Aws4yDwcwyDgYzyzgYzCzjYDCzjIPBzDIOBjPLOBjMLONgMLOMg8HMMg4GM8s4GMws42Aws4yDwcwyDgYzyzgYzCzjYDCzjIPBzDIOBjPLOBjMLONgMLOMg8HMMg4GM8s4GMws42Aws4yDwcwyRQeDpBpJGyQ9n+YnSHpF0g5Jv5Q0OPVfkuZ3pOXNJardzErkbPYYvgFs6zX/KDA/Ij4EtAFzUv8coC31z0/jzKyKFBUMkpqAzwBPpXkBNwG/SkMWAbel6RlpnrT8U2m8mVWJYvcYvg/cB3Sl+ZHA4Yg4meZbgHFpehywFyAtb0/jTyNprqR1ktadW+lmVip9BoOkzwIHImJ9f244IhZGxLSImNaf72tm56+2iDHXA7dKugWoB4YDPwBGSKpNewVNwL40fh8wHmiRVAtcChzq98rNrGT63GOIiAcioikimoFZwIsRMRt4CZiZht0FLEvTy9M8afmLERH9WrWZldT5XMdwP/AtSTsonEN4OvU/DYxM/d8C5p1fiWY20FQJv8wllb8Iswvf+mLP6fnKRzPLOBjMLONgMLOMg8HMMg4GM8s4GMws42Aws4yDwcwyDgYzyzgYzCzjYDCzjIPBzDIOBjPLOBjMLONgMLOMg8HMMg4GM8s4GMws42Aws4yDwcwyDgYzyzgYzCzjYDCzjIPBzDIOBjPLOBjMLONgMLOMg8HMMg4GM8s4GMws42Aws4yDwcwyDgYzyzgYzCxTVDBI2i1pk6SNktalvsslrZK0Pb1elvol6YeSdkh6TdKUUn4BZtb/zmaP4caImBwR09L8PGB1REwEVqd5gE8DE1ObCzzZX8Wa2cA4n0OJGcCiNL0IuK1X/+Io+D0wQtLY89iOmQ2wYoMhgBckrZc0N/U1RsRbaXo/0JimxwF7e63bkvpOI2mupHXdhyZmVjlqixz3iYjYJ2kMsErS//VeGBEhKc5mwxGxEFgIcLbrmllpFbXHEBH70usB4DngI0Br9yFCej2Qhu8DxvdavSn1mVmV6DMYJP2VpGHd08DfA5uB5cBdadhdwLI0vRy4M306cR3Q3uuQw8yqQDGHEo3Ac5K6x/88Iv5X0lpgiaQ5wBvAF9L43wC3ADuAo8CX+71qMyspRZT/8F7Sn4HXy11HkUYBB8tdRBGqpU6onlqrpU5491r/OiJGF7NysScfS+31XtdHVDRJ66qh1mqpE6qn1mqpE86/Vl8SbWYZB4OZZSolGBaWu4CzUC21VkudUD21VkudcJ61VsTJRzOrLJWyx2BmFaTswSBpuqTX023a8/peo6S1/ETSAUmbe/VV5O3lksZLeknSVklbJH2jEuuVVC/pD5L+mOr899Q/QdIrqZ5fShqc+i9J8zvS8uaBqLNXvTWSNkh6vsLrLO2jECKibA2oAXYCVwCDgT8Ck8pYz98BU4DNvfoeA+al6XnAo2n6FmAFIOA64JUBrnUsMCVNDwP+BEyqtHrT9hrSdB3wStr+EmBW6l8A/GOa/idgQZqeBfxygP9dvwX8HHg+zVdqnbuBUWf09dv//YB9Ie/xxX0MWNlr/gHggTLX1HxGMLwOjE3TYylccwHwn8AX321cmepeBtxcyfUCQ4FXgY9SuPim9szvA2Al8LE0XZvGaYDqa6LwbJGbgOfTD1LF1Zm2+W7B0G//9+U+lCjqFu0yO6/bywdC2o39Wwq/jSuu3rR7vpHCjXarKOwlHo6Ik+9SS0+daXk7MHIg6gS+D9wHdKX5kRVaJ5TgUQi9VcqVj1Uh4uxvLy81SQ3AUuCbEfFOuqcFqJx6I+IUMFnSCAp3515V3opykj4LHIiI9ZJuKHM5xej3RyH0Vu49hmq4Rbtiby+XVEchFH4WEc+m7oqtNyIOAy9R2CUfIan7F1PvWnrqTMsvBQ4NQHnXA7dK2g08Q+Fw4gcVWCdQ+kchlDsY1gIT05nfwRRO4iwvc01nqsjby1XYNXga2BYRT1RqvZJGpz0FJA2hcB5kG4WAmPkedXbXPxN4MdKBcSlFxAMR0RQRzRS+D1+MiNmVVicM0KMQBupkyfucRLmFwhn1ncCDZa7lF8BbQCeF47A5FI4bVwPbgd8Cl6exAn6U6t4ETBvgWj9B4TjzNWBjardUWr3AtcCGVOdm4F9T/xXAHyjcnv8/wCWpvz7N70jLryjD98EN/OVTiYqrM9X0x9S2dP/c9Of/va98NLNMuQ8lzKwCORjMLONgMLOMg8HMMg4GM8s4GMws42Aws4yDwcwy/w+yWP++WUuIKQAAAABJRU5ErkJggg==\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"import segmentation_models_pytorch as smp","metadata":{"execution":{"iopub.status.busy":"2022-09-23T14:39:14.904865Z","iopub.execute_input":"2022-09-23T14:39:14.905421Z","iopub.status.idle":"2022-09-23T14:39:17.910039Z","shell.execute_reply.started":"2022-09-23T14:39:14.905377Z","shell.execute_reply":"2022-09-23T14:39:17.908932Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"**Create segmentation model with pretrained encoder**","metadata":{}},{"cell_type":"code","source":"ENCODER = 'resnext101_32x4d'#'tu-xception71' #'resnet101'\nENCODER_WEIGHTS = 'ssl' #'imagenet'#uncomment if xception or resnet101\nACTIVATION = 'sigmoid' \n\nmodel = smp.DeepLabV3Plus(\n     encoder_name=ENCODER, \n     encoder_weights=ENCODER_WEIGHTS, \n     classes=1, \n     activation=ACTIVATION,\n )","metadata":{"execution":{"iopub.status.busy":"2022-09-23T14:39:17.912272Z","iopub.execute_input":"2022-09-23T14:39:17.912622Z","iopub.status.idle":"2022-09-23T14:39:21.772643Z","shell.execute_reply.started":"2022-09-23T14:39:17.912590Z","shell.execute_reply":"2022-09-23T14:39:21.771423Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stderr","text":"Downloading: \"https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnext101_32x4-dc43570a.pth\" to /root/.cache/torch/hub/checkpoints/semi_supervised_resnext101_32x4-dc43570a.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/169M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b7962ff5af946538faa9cf18f787644"}},"metadata":{}}]},{"cell_type":"code","source":"from segmentation_models_pytorch.utils import *","metadata":{"execution":{"iopub.status.busy":"2022-09-23T14:39:21.775716Z","iopub.execute_input":"2022-09-23T14:39:21.776956Z","iopub.status.idle":"2022-09-23T14:39:21.790001Z","shell.execute_reply.started":"2022-09-23T14:39:21.776904Z","shell.execute_reply":"2022-09-23T14:39:21.787986Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"**Losses**","metadata":{}},{"cell_type":"code","source":"epsilon = 1e-5\nsmooth = 1\nDiceLoss=smp.utils.losses.DiceLoss()\n\ndef tversky(y_pred, y_true):\n    y_true_pos = torch.flatten(y_true)\n    y_pred_pos = torch.flatten(y_pred)\n    true_pos = torch.sum(y_true_pos * y_pred_pos)\n    false_neg = torch.sum(y_true_pos * (1-y_pred_pos))\n    false_pos = torch.sum((1-y_true_pos)*y_pred_pos)\n    alpha = 0.7\n    return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n\ndef tversky_loss( y_pred,y_true):\n    return 1 - tversky(y_true,y_pred)\n\ndef focal_tversky(y_pred,y_true):\n    pt_1 = tversky(y_pred,y_true)\n    gamma = 0.75\n    return torch.pow((1-pt_1), gamma)\nclass Focal_Tversky(base.Loss):\n    def __init__(self, eps=1.0, activation=None, ignore_channels=None, **kwargs):\n        super().__init__(**kwargs)\n        self.eps = eps\n        #self.activation = Activation(activation)\n        self.ignore_channels = ignore_channels\n\n    def forward(self, y_pr, y_gt):\n        #y_pr = self.activation(y_pr)\n        return 0.5*focal_tversky(y_pr,y_gt) + 0.5*DiceLoss(y_pr,y_gt)","metadata":{"execution":{"iopub.status.busy":"2022-09-23T14:42:45.446957Z","iopub.execute_input":"2022-09-23T14:42:45.447553Z","iopub.status.idle":"2022-09-23T14:42:45.461649Z","shell.execute_reply.started":"2022-09-23T14:42:45.447504Z","shell.execute_reply":"2022-09-23T14:42:45.460114Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"writer = SummaryWriter('dlv3+')","metadata":{"execution":{"iopub.status.busy":"2022-09-23T14:42:45.941678Z","iopub.execute_input":"2022-09-23T14:42:45.942581Z","iopub.status.idle":"2022-09-23T14:42:45.950050Z","shell.execute_reply.started":"2022-09-23T14:42:45.942522Z","shell.execute_reply":"2022-09-23T14:42:45.948740Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# Set flag to train the model or not. If set to 'False', only prediction is performed (using an older model checkpoint)\nTRAINING = True\n\n# Set number of epochs\nEPOCHS = 50\n\n# Set device: `cuda` or `cpu`\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n#Loss Function\nloss = Focal_Tversky()\n\n#Metrics\nmetrics = [\n    smp.utils.metrics.IoU(threshold=0.5),\n]\n\n#Optimizer\noptimizer = torch.optim.Adam([ \n    dict(params=model.parameters(), lr=0.0001),\n])","metadata":{"execution":{"iopub.status.busy":"2022-09-23T14:42:46.427093Z","iopub.execute_input":"2022-09-23T14:42:46.428269Z","iopub.status.idle":"2022-09-23T14:42:46.438281Z","shell.execute_reply.started":"2022-09-23T14:42:46.428227Z","shell.execute_reply":"2022-09-23T14:42:46.436856Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"**Creating training and validation epoch**","metadata":{}},{"cell_type":"code","source":"train_epoch = smp.utils.train.TrainEpoch(\n    model, \n    loss=loss, \n    metrics=metrics, \n    optimizer=optimizer,\n    device=DEVICE,\n    verbose=True,\n)\n\nvalid_epoch = smp.utils.train.ValidEpoch(\n    model, \n    loss=loss, \n    metrics=metrics, \n    device=DEVICE,\n    verbose=True,\n)","metadata":{"execution":{"iopub.status.busy":"2022-09-23T14:42:47.822275Z","iopub.execute_input":"2022-09-23T14:42:47.823074Z","iopub.status.idle":"2022-09-23T14:42:47.844816Z","shell.execute_reply.started":"2022-09-23T14:42:47.823031Z","shell.execute_reply":"2022-09-23T14:42:47.843453Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-09-23T14:42:48.706189Z","iopub.execute_input":"2022-09-23T14:42:48.706717Z","iopub.status.idle":"2022-09-23T14:42:48.713128Z","shell.execute_reply.started":"2022-09-23T14:42:48.706675Z","shell.execute_reply":"2022-09-23T14:42:48.711520Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"**Train the model**","metadata":{}},{"cell_type":"code","source":"%%time\n\nif TRAINING:\n\n    best_iou_score = 0.0\n    train_logs_list, valid_logs_list = [], []\n\n    for i in range(0, EPOCHS):\n\n        # Perform training & validation\n        print('\\nEpoch: {}'.format(i))\n        train_logs = train_epoch.run(dataloader_training)\n        valid_logs = valid_epoch.run(dataloader_validation)\n        train_logs_list.append(train_logs)\n        writer.add_scalar(\"Loss/train\", train_logs['focal__tversky'], i)\n        writer.add_scalar(\"IOU/train\", train_logs['iou_score'], i)\n        valid_logs_list.append(valid_logs)\n        writer.add_scalar(\"Loss/valid\", valid_logs['focal__tversky'], i)\n        writer.add_scalar(\"IOU/valid\", valid_logs['iou_score'], i)\n\n        #Save model if a better val IoU score is obtained\n        if best_iou_score < valid_logs['iou_score']:\n            best_iou_score = valid_logs['iou_score']\n            torch.save(model, './best_modelv3plus.pth')\n            print('Model saved!')\nwriter.flush()","metadata":{"execution":{"iopub.status.busy":"2022-09-23T14:29:59.587196Z","iopub.status.idle":"2022-09-23T14:29:59.587638Z","shell.execute_reply.started":"2022-09-23T14:29:59.587443Z","shell.execute_reply":"2022-09-23T14:29:59.587463Z"},"trusted":true},"execution_count":null,"outputs":[]}]}