{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom skimage.io import imread\nfrom torch.utils import data\nfrom torch.utils.data import DataLoader\nfrom sklearn.model_selection import train_test_split\nimport pathlib\nimport torchvision.transforms.functional as fn\nfrom torchvision.io import read_image\nfrom torchvision import transforms\nimport numpy as np","metadata":{"id":"gmvIP-mgL6Q3","execution":{"iopub.status.busy":"2022-09-20T18:19:08.043860Z","iopub.execute_input":"2022-09-20T18:19:08.044208Z","iopub.status.idle":"2022-09-20T18:19:11.772863Z","shell.execute_reply.started":"2022-09-20T18:19:08.044120Z","shell.execute_reply":"2022-09-20T18:19:11.771722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nimport cv2 as cv","metadata":{"id":"X0vVbQeWMDQ-","execution":{"iopub.status.busy":"2022-09-20T18:19:11.839497Z","iopub.execute_input":"2022-09-20T18:19:11.840431Z","iopub.status.idle":"2022-09-20T18:19:12.078874Z","shell.execute_reply.started":"2022-09-20T18:19:11.840385Z","shell.execute_reply":"2022-09-20T18:19:12.077676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Grayscale()\nfrom tqdm import tqdm\n\n\nclass SegmentationDataSet2(data.Dataset):\n    \"\"\"Image segmentation dataset with caching and pretransforms.\"\"\"\n    def __init__(self,\n                 inputs: list,\n                 targets: list,\n                 transform=None,\n                 use_cache=False,\n                 pre_transform=None,\n                 ):\n        self.inputs = inputs\n        self.targets = targets\n        self.transform = transform\n        self.inputs_dtype = torch.float32\n        self.targets_dtype = torch.long\n        self.use_cache = use_cache\n        self.pre_transform = pre_transform\n\n        if self.use_cache:\n            self.cached_data = []\n\n            progressbar = tqdm(range(len(self.inputs)), desc='Caching')\n            for i, img_name, tar_name in zip(progressbar, self.inputs, self.targets):\n                img, tar = imread(str(img_name)), imread(str(tar_name))\n                if self.pre_transform is not None:\n                    img, tar = self.pre_transform(img, tar)\n\n                self.cached_data.append((img, tar))\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self,\n                    index: int):\n        if self.use_cache:\n            x, y = self.cached_data[index]\n        else:\n            # Select the sample\n            input_ID = self.inputs[index]\n            target_ID = self.targets[index]\n\n            # Load input and target\n            x, y = read_image(input_ID), read_image(target_ID)\n            x, y = x.type(self.inputs_dtype), y.type(self.targets_dtype)\n            x = (x - torch.min(x)) / (torch.max(x)-torch.min(x))\n            y = transform(y)\n            x,y = fn.resize(x,size=[512,512]),fn.resize(y,size=[512,512])\n            y[y>0]=1\n            y.squeeze()\n\n        # Preprocessing\n        if self.transform is not None:\n            x, y = self.transform(x, y)\n\n        \n        \n\n        return x, y","metadata":{"id":"x7bFl1oiMKXo","execution":{"iopub.status.busy":"2022-09-20T18:19:12.100333Z","iopub.execute_input":"2022-09-20T18:19:12.101386Z","iopub.status.idle":"2022-09-20T18:19:12.278347Z","shell.execute_reply.started":"2022-09-20T18:19:12.101351Z","shell.execute_reply":"2022-09-20T18:19:12.277135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install segmentation-models-pytorch","metadata":{"id":"SsgOLFuDMMuH","outputId":"b875e461-59db-462d-c46a-897ef6ccade1","execution":{"iopub.status.busy":"2022-09-20T18:19:12.279888Z","iopub.execute_input":"2022-09-20T18:19:12.280311Z","iopub.status.idle":"2022-09-20T18:19:28.315881Z","shell.execute_reply.started":"2022-09-20T18:19:12.280275Z","shell.execute_reply":"2022-09-20T18:19:28.314716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = []\ndir1 =  '../input/contrail-dataset-new/FinalData/FalseColor'\nfor root, dirs, files in os.walk(dir1):\n    inputs.extend(files)\ninputs = sorted(inputs)\n\nrgbs = []\ndir3 = '../input/contrail-dataset-new/FinalData/RGB'\nfor root, dirs, files in os.walk(dir3):\n    rgbs.extend(files)\nrgbs = sorted(rgbs)\n# read annotations\ntarget = []\ndir2 = '../input/contrail-dataset-new/FinalData/Contrail'\nfor root, dirs, files in os.walk(dir2):\n    target.extend(files)\ntarget = sorted(target)\n\nr_g_b = []\ninp = []\ntar = []\nfor x in inputs:\n    inp.append('../input/contrail-dataset-new/FinalData/FalseColor/'+x)\nfor x in target:\n    tar.append('../input/contrail-dataset-new/FinalData/Contrail/'+x)\nfor x in rgbs:\n    r_g_b.append('../input/contrail-dataset-new/FinalData/RGB'+x)\n# inputs = inp\n# target = tar\n","metadata":{"id":"KE2irrTxMP33","execution":{"iopub.status.busy":"2022-09-20T18:19:28.319729Z","iopub.execute_input":"2022-09-20T18:19:28.320065Z","iopub.status.idle":"2022-09-20T18:19:40.331458Z","shell.execute_reply.started":"2022-09-20T18:19:28.320031Z","shell.execute_reply":"2022-09-20T18:19:40.330291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_true = []\ntarget_true = []\nrgb_true = []\nfor count, value in enumerate(tar):\n    y = read_image(value)\n    y = transform(y)\n    y[y>0]=1 \n    if(len(y.unique())>1):\n        input_true.append(inp[count])\n        target_true.append(tar[count])\n        rgb_true.append(r_g_b[count])\n    if(count%100==0):\n        print(count)\n          ","metadata":{"id":"OIaNvGRIMTtd","execution":{"iopub.status.busy":"2022-09-20T18:19:40.332949Z","iopub.execute_input":"2022-09-20T18:19:40.333323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter('dlv3except')\n\nrandom_seed = 60\n\n\n\n# split dataset into training set and validation set\ntrain_size = 0.8  # 80:20 split\n\ninputs_train, inputs_valid = train_test_split(\n    input_true,\n    random_state=random_seed,\n    train_size=train_size,\n    shuffle=True)\n\ntargets_train, targets_valid = train_test_split(\n    target_true,\n    random_state=random_seed,\n    train_size=train_size,\n    shuffle=True)\n\nrgb_train, rgb_valid = train_test_split(\n    rgb_true,\n    random_state=random_seed,\n    train_size = train_size,\n    shuffle=True)\n# dataset training\ndataset_train = SegmentationDataSet2(inputs=inputs_train,\n                                    targets=targets_train,\n                                    transform=None)\n\n# dataset validation\ndataset_valid = SegmentationDataSet2(inputs=inputs_valid,\n                                    targets=targets_valid,\n                                    transform=None)\n\n\ndataset_rgb =  SegmentationDataSet2(inputs=rgb_valid,\n                                    targets=targets_valid,\n                                    transform=None)\n\n# dataloader training\ndataloader_training = DataLoader(dataset=dataset_train,\n                                 batch_size=1,\n                                 shuffle=True)\n\n# dataloader validation\ndataloader_validation = DataLoader(dataset=dataset_valid,\n                                   batch_size=1,\n                                   shuffle=False)","metadata":{"id":"aE9GfN1_MZtK","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Trying out the data loaded\nbatch = dataset_train[114]\nx, y = batch\n\nprint(f'x = shape: {x.shape}; type: {x.dtype}')\nprint(f'x = min: {x.min()}; max: {x.max()}')\nprint(f'y = shape: {y.shape}; class: {y.unique()}; type: {y.dtype}')\nprint(f'y = min: {y.min()}; max: {y.max()}')\n#Getting the next batch::  x, y = next(iter(dataloader_training))","metadata":{"id":"Bu-Kr1rVMlr3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install albumentations\nimport numpy as np\nimport albumentations as album\n!pip install -q -U segmentation-models-pytorch albumentations > /dev/null\nimport segmentation_models_pytorch as smp\n!pip install --upgrade certifi\nimport re\nfrom segmentation_models_pytorch.utils import *","metadata":{"id":"hz_z5DigMsEJ","outputId":"4a68a662-c1fc-4a6a-eecd-3a095dd25774","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nfrom segmentation_models_pytorch.utils import *","metadata":{"id":"4t1nUHggNGzS","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Set device: `cuda` or `cpu`\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#load best saved models from previouscheckpoint \n\nmodel1 = torch.load('../input/contrail-segment-models/Unet_Xception.pth', map_location=DEVICE)\nprint('Loaded model 1 from this run.')\nmodel2 = torch.load('../input/contrail-segment-models/PSP Net with Resnext101.pth', map_location=DEVICE)\nprint('Loaded model 2 from this run.')\nmodel3 = torch.load('../input/contrail-segment-models/best_modelv3plus resnext.pth', map_location=DEVICE)\nprint('Loaded model 3 from this run.')\nmodel4 = torch.load('../input/contrail-segment-models/dlv3resnext.pth', map_location=DEVICE)\nprint('Loaded model 4 from this run.')","metadata":{"id":"lpu1qUMgNPew","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\nfrom torchmetrics import JaccardIndex\n# iou = JaccardIndex(num_classes=1,average=None).to(device)\niou = smp.utils.metrics.IoU(threshold=0.5)\nclass Trainer:\n    def __init__(self,\n                 model1: torch.nn.Module,\n                 model2: torch.nn.Module,\n                 model3: torch.nn.Module,\n                 model4: torch.nn.Module,\n                 mode,\n                 device: torch.device,\n                 training_DataLoader: torch.utils.data.Dataset=None,\n                 validation_DataLoader: torch.utils.data.Dataset = None,\n                 epochs: int = 100,\n                 epoch: int = 0,\n                 notebook: bool = False\n                 ):\n\n        self.model1 = model1\n        self.model2= model2\n        self.model3 = model3\n        self.model4 = model4\n        self.training_DataLoader = training_DataLoader\n        self.validation_DataLoader = validation_DataLoader\n        self.device = device\n        self.epochs = epochs\n        self.epoch = epoch\n        self.notebook = notebook\n        self.MiouT = []\n        self.MiouV = []\n        self.mode = mode\n\n    def run_trainer(self):\n\n        if self.notebook:\n            from tqdm.notebook import tqdm, trange\n        else:\n            from tqdm import tqdm, trange\n\n        progressbar = trange(self.epochs, desc='Progress')\n        for i in progressbar:\n            \"\"\"Epoch counter\"\"\"\n            self.epoch += 1  # epoch counter\n\n            \"\"\"Training block\"\"\"\n            if self.training_DataLoader is not None:\n                self._train()\n\n            \"\"\"Validation block\"\"\"\n            if self.validation_DataLoader is not None:\n                self._validate()\n        return self.MiouV , self.MiouT\n\n    def _train(self):\n\n        if self.notebook:\n            from tqdm.notebook import tqdm, trange\n        else:\n            from tqdm import tqdm, trange\n\n        self.model1.eval()\n        self.model2.eval()\n        self.model3.eval()\n        self.model4.eval()# evaluation mode\n        miou_list = []\n        batch_iter = tqdm(enumerate(self.training_DataLoader), 'Training', total=len(self.training_DataLoader),\n                          leave=False)\n\n        for i, (x, y) in batch_iter:\n            inpt, target = x.to(self.device), y.to(self.device)  # send to device (GPU or CPU)\n            del(x,y)\n            with torch.no_grad():\n                out1 = self.model1(inpt)\n                out2 = self.model2(inpt)\n                out3 = self.model3(inpt)\n                out4 = self.model4(inpt)\n                out1 = out1.squeeze()\n                out2 = out2.squeeze()\n                out3 = out3.squeeze()\n                out4 = out4.squeeze()\n                if self.mode == torch.mode():\n                    out1[out1>=0.5]=1\n                    out1[out1<1]=0\n                    out2[out2>=0.5]=1\n                    out2[out2<1]=0\n                    out3[out3>=0.5]=1\n                    out3[out3<1]=0\n                    out4[out4>=0.5]=1\n                    out4[out4<1]=0\n                    out = torch.stack((out1,out2,out3,out4))\n                    out = self.mode(out,dim=0)\n                else:\n                    out = torch.stack((out1,out2,out3,out4))\n                    out = self.mode(out,dim=0)\n                out=out.values\n                out = out.reshape((1,1,512,512))\n                miou = iou(out,target)\n                miou = miou.item()\n                miou_list.append(miou)\n                del(out)\n                batch_iter.set_description(f'training: Miou:{miou}')\n        self.MiouT.append(np.mean(miou_list))\n        batch_iter.close()\n\n    def _validate(self):\n\n        if self.notebook:\n            from tqdm.notebook import tqdm, trange\n        else:\n            from tqdm import tqdm, trange\n\n        self.model1.eval()\n        self.model2.eval()\n        self.model3.eval()\n        self.model4.eval()# evaluation mode\n        miou_listV = []\n        batch_iter = tqdm(enumerate(self.validation_DataLoader), 'Validation', total=len(self.validation_DataLoader),\n                          leave=False)\n\n        for i, (x, y) in batch_iter:\n            inpt, target = x.to(self.device), y.to(self.device)  # send to device (GPU or CPU)\n            del(x,y)\n            with torch.no_grad():\n                out1 = self.model1(inpt)\n                out2 = self.model2(inpt)\n                out3 = self.model3(inpt)\n                out4 = self.model4(inpt)\n                out1 = out1.squeeze()\n                out2 = out2.squeeze()\n                out3 = out3.squeeze()\n                out4 = out4.squeeze()\n                if self.mode == torch.mode():\n                    out1[out1>=0.5]=1\n                    out1[out1<1]=0\n                    out2[out2>=0.5]=1\n                    out2[out2<1]=0\n                    out3[out3>=0.5]=1\n                    out3[out3<1]=0\n                    out4[out4>=0.5]=1\n                    out4[out4<1]=0\n                    out = torch.stack((out1,out2,out3,out4))\n                    out = self.mode(out,dim=0)\n                else:\n                    out = torch.stack((out1,out2,out3,out4))\n                    out = self.mode(out,dim=0)\n                out=out.values\n                out = out.reshape((1,1,512,512))\n                miou = iou(out,target)\n                miou = miou.item()\n                miou_listV.append(miou)\n                del(out)\n                batch_iter.set_description(f'Validation: Miou:{miou}')\n        self.MiouV.append(np.mean(miou_listV))\n        batch_iter.close()","metadata":{"id":"iRCaUGDTNlIq","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(model1=model1,\n                  model2=model2,\n                  model3=model3,\n                  model4=model4,\n                  mode=torch.mode(),##or torch.mean()##or torch.max()\n                  device=device,\n                #  training_DataLoader=dataloader_training,\n                  validation_DataLoader=dataloader_validation,\n                  epochs=1,\n                  epoch=0,\n                  notebook=True)\nvalid_iou, train_iou= trainer.run_trainer()","metadata":{"id":"e9DQZsOMNycR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_iou","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_iou","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index = 29\ndef visualize(**images):\n    \"\"\"\n    Plot images in one row\n    \"\"\"\n    n_images = len(images)\n    plt.figure(figsize=(20,8))\n    for idx, (name, image) in enumerate(images.items()):\n        plt.subplot(1, n_images, idx + 1)\n        plt.xticks([]); \n        plt.yticks([])\n        # get title from the parameter names\n        plt.title(name.replace('_',' ').title(), fontsize=20)\n        plt.imshow(image)\n        image1 = np.asarray(image)\n        name = '/notebooks/ImagesFinal/'+name\n        name = name + str(index) + '.png'\n        plt.imsave(name,image1)\n    plt.show()\n","metadata":{"id":"bOhy5oGbOHn6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image, gt_mask = dataset_valid[index]\ngt_mask = gt_mask.squeeze()\nimage1=np.asarray(image)\nx_tensor = torch.from_numpy(image1).to(DEVICE).unsqueeze(0)\nx_tensor.shape\npr_mask = model.predict(x_tensor)\npr_mask = (pr_mask.squeeze().cpu().numpy().round())\n\ngt_mask_final=gt_mask.reshape(512,512)\n#y[1]\nimage_final=image.permute(1, 2, 0)\npr_mask_final=pr_mask.reshape(512,512)\nvisualize(\n        False_Colour = image_final,\n        ground_truth_mask = gt_mask_final,\n        predicted_mask = pr_mask_final\n    )","metadata":{"id":"3DZEmov0OLSl","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image, mask = dataset_rgb[index]\nmask = mask.squeeze()\nimage1=np.asarray(image)\nmask_final = mask.squeeze()\n# image=np.asarray(image)\nimage_final=image.permute(1, 2, 0)\nvisualize(\n        RGB = image_final,\n        ground_truth_mask_check = mask_final,\n    )","metadata":{"id":"P4OjLfcJO9aY","trusted":true},"execution_count":null,"outputs":[]}]}